---
layout: post
title: Two Layer Neural Networks
---
Let $\theta = (W,a,b)$. Then we define the two layer neural network $f_\theta$ by:

$$
f_\theta(x) := a^T \sigma(W x + b)
$$

## Expanding the Gradient
Because $f_{\theta}(x) \approx 0$ at initialization,

$$
\begin{align*}
    \nabla_{w_j} L(\theta)
    &= a_jE_{x,y}[(f_\theta(x)-y)\sigma'(w_j \cdot x)x] \\
    &\approx -a_j E_{x,y}[y\sigma'(w_j \cdot x)x]
\end{align*}
$$

## Video Test:

### Video with controls (html5)
<center>
<video width="100%" controls>
    <source src="/assets/muP.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>
</center>

### Video without controls (markdown)
![Alt Text](/assets/muP.mp4)

## Gif Test:
![Alt Text](/assets/animation.gif)