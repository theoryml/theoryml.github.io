---
layout: post
title: The Implicit Bias of SGD
author: Alex Damian
---
# Notation

Stochastic gradient descent and its variants (e.g. ADAM) are the workhorses of modern machine learning. The general form of stochastic gradient descent is

$$
\theta \leftarrow \theta - \eta \widehat g(\theta)
$$

where $\widehat g(\theta)$ is a stochastic estimate of the gradient of the loss function $\mathcal{L}(\theta)$. We assume that $\widehat g(\theta)$ is an unbiased estimate of the true gradient, i.e. $\mathbb E[\widehat g(\theta)] = \nabla_\theta \mathcal L(\theta)$. For example, $\widehat g(\theta)$ could be the gradient of the loss function on a random minibatch of data points.

If we define $z := \widehat g(\theta) - \nabla_\theta \mathcal L(\theta)$ to be the mean zero gradient noise, then we can rewrite the update as:

$$
\theta \leftarrow \theta - \eta \left[ \nabla_\theta \mathcal{L}(\theta) + z \right]
$$

# SGD on a Quadratic

In order to understand the implicit regularization effect of SGD in the overparameterized regime, we must begin by studying the behavior of SGD when the loss is quadratic and the noise covariance is constant. We will begin with the univariate case and then generalize to the high dimensional setting.

## Univariate Case

Let $\theta \in \mathbb{R}$ and let $L(\theta) = \frac{1}{2} \theta^2$. Furthermore, we will assume that the noise $z_t = \widehat{g}(\theta_t) - \nabla_\theta \mathcal L(\theta_t)$ is a Gaussian random variable with mean zero and variance $\sigma^2$. We can now explicitly write the gradient update:

$$
\begin{align*}
    \theta_{t+1} &= \theta_t - \eta \left[ \theta_t + z_t \right] \\
    &= \underbrace{(1 - \eta) \theta_t}_{\text{contraction}} - \underbrace{\eta z_t}_{\text{noise}}.
\end{align*}
$$

The first term is responsible for shrinking $\theta$ towards the global minimum at $0$ while the second term is responsible for random fluctuations. This process is a *discrete time [Ornstein-Uhlenbeck process](https://en.wikipedia.org/wiki/Ornsteinâ€“Uhlenbeck_process)*, also known as an [$AR(1)$ process](https://en.wikipedia.org/wiki/Autoregressive_model). This process has two important properties:
1. For any $t$, $\theta_t$ is Gaussian.
2. $\theta_t$ is ergodic and mixes to a stationary distribution $N(0,S)$.

To see these, we can directly expand the recurrence for $\theta_t$:

$$
\theta_t = (1 - \eta)^t \theta_0 - \eta \sum_{i=0}^{t-1} (1 - \eta)^i z_{t-i-1}.
$$

Assuming that $\eta \in (0,2)$, the first term will converge to $0$ exponentially quickly. However, because $z_t$ is Gaussian and the sum of multiple Gaussians remains Gaussian, the second term will converge to a Gaussian random variable with mean zero and variance

$$
\begin{align*}
    S
    &= \eta^2 \sum_{i=0}^{t-1} (1 - \eta)^{2i} \sigma^2 \\
    &= \eta^2 \sigma^2 \sum_{i=0}^{t-1} \left[ (1 - \eta)^2 \right]^i \\
    &= \eta^2 \sigma^2 \cdot \frac{1 - (1 - \eta)^{2t}}{1 - (1 - \eta)^2} \\
    &\xrightarrow{t \to \infty} \frac{\eta \sigma^2}{2 - \eta}.
\end{align*}
$$

Therefore, as $t \to \infty$, the distribution of $\theta_t$ converges to $\mathcal{N}(0,S)$ where

$$
S = \frac{\eta \sigma^2}{2 - \eta}.
$$

## High Dimensional Case

Let $H$ denote the Hessian of the loss so that

$$
\mathcal{L}(\theta) = \frac{1}{2} \theta^T H \theta.
$$

Furthermore, we will assume that $z_t = \widehat g(\theta_t) - \nabla_\theta \mathcal L(\theta_t)$ is a Gaussian random variable with mean zero and covariance $\Sigma$. We will assume that $\Sigma$ is constant, i.e. it does not depend on $\theta_t$. We can now explicitly write the gradient update:

$$
\begin{align*}
    \theta_{t+1} &= \theta_t - \eta \left[ H \theta_t + z_t \right] \\
    &= \underbrace{(I - \eta H)\theta_t}_{\text{contraction}} - \underbrace{\eta z_t}_{\text{noise}}.
\end{align*}
$$

As above, this is an example of a discrete time Ornstein-Uhlenbeck process in high dimensions. The same analysis as above shows that $\theta_t$ is Gaussian for all $t$ and that $\theta_t$ converges to a stationary distribution $N(0,S)$. To solve for $S$, we will directly analyze the fixed point of the recurrence. If $\theta_t$ is stationary with $\theta_t \sim \mathcal{N}(0,S)$ then

$$
\theta_{t+1} \sim \mathcal{N}\left(0,(I - \eta H)S(I - \eta H) + \eta^2 \Sigma\right).
$$

Therefore the variance of the stationary distribution must be a solution to the fixed point equation:

$$
S = (I - \eta H) S (I - \eta H) + \eta^2 \Sigma
$$

This is an example of the [discrete time Lyapunov equation](https://en.wikipedia.org/wiki/Lyapunov_equation). When $I - \eta H$ is a strict contraction, i.e. the eigenvalues of $H$ are between $0$ and $2/\eta$, then this equation has a unique solution $S$. However, analytically solving for $S$ in terms of $H,\Sigma$ is somewhat complicated.

However, in some special cases we can still analytically solve this equation. For example, in the special case of label noise SGD [CITE], we have that $\Sigma \approx \sigma^2 H$ where $\sigma^2$ is the strength of the label noise. In this case, the variance of the stationary distribution has the closed form

$$
S = \eta \sigma^2 (2 - \eta H)^{-1} \Pi_{\mathrm{span}(H)}
$$

where $\Pi_{\mathrm{span}(H)}$ denotes the orthogonal projection onto the span of $H$. Note that this mirrors the univariate case. Furthermore, when $\eta \lambda_{max}(H) \ll 1$, we can approximate $(2 - \eta H)^{-1} \approx \frac{1}{2}I$ so that

$$
S \approx \frac{\eta \sigma^2}{2} \Pi_{\mathrm{span}(H)}
$$

so that the stationary distribution is approximately an isotropic Gaussian restricted to the span of the Hessian, $H$.